--- crhoy_scraper.py
+++ crhoy_scraper.py
@@
-def scrape_crhoy_for_date(date_str: str) -> List[Article]:
+def scrape_crhoy_for_date(date_str: str, limit: Optional[int] = 5) -> List[Article]:
     """
     Scrape CRHoy articles for a single date.
 
     :param date_str: YYYY-MM-DD
-    :returns: up to 5 articles
+    :param limit: max articles to fetch (None => no limit)
     """
     # … your existing setup …
 
-    # you probably have something like:
-    for url in urls[:5]:
+    # now honor the `limit` param:
+    urls_to_fetch = urls if (limit is None) else urls[:limit]
+    for url in urls_to_fetch:
         article = fetch_article(url)
         articles.append(article)
 
     return articles
+
+
+def scrape_crhoy_for_date_range(
+    start_date_str: str,
+    end_date_str: str,
+    limit_per_day: Optional[int] = None
+) -> List[Article]:
+    """
+    Scrape CRHoy articles for every date in [start_date, end_date].
+
+    :param start_date_str: YYYY-MM-DD
+    :param end_date_str:   YYYY-MM-DD
+    :param limit_per_day:  max per day (None => all)
+    """
+    start = datetime.strptime(start_date_str, "%Y-%m-%d").date()
+    end   = datetime.strptime(end_date_str,   "%Y-%m-%d").date()
+    all_articles: List[Article] = []
+
+    current = start
+    while current <= end:
+        day = current.strftime("%Y-%m-%d")
+        print(f"→ Scraping {day}")
+        today = scrape_crhoy_for_date(day, limit=limit_per_day)
+        all_articles.extend(today)
+
+        # polite delay
+        time.sleep(random.uniform(1, 3))
+        current += timedelta(days=1)
+
+    return all_articles